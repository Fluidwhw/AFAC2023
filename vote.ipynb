{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图片已载入，共 20000 张图片\n",
      "图片已载入，共 3000 张图片\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/8 开始 ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 108\u001b[0m\n\u001b[0;32m    106\u001b[0m running_mae \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    107\u001b[0m all_errors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 108\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[2], line 53\u001b[0m, in \u001b[0;36mAgeDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# 应用图像变换\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 53\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, age\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\transforms\\functional.py:477\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    475\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:2222\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2210\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2211\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2212\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[0;32m   2213\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2214\u001b[0m         )\n\u001b[0;32m   2215\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2216\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2217\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2218\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2219\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2220\u001b[0m         )\n\u001b[1;32m-> 2222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# 定义数据集类\n",
    "class AgeDataset(Dataset):\n",
    "    def __init__(self, image_dir, csv_file, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_age_mapping = {}\n",
    "\n",
    "        # 读取 CSV 文件，构建图片名称到年龄的映射\n",
    "        with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                image_name = row['name']\n",
    "                age = int(row['age'])\n",
    "                self.image_age_mapping[image_name] = age\n",
    "\n",
    "        # 获取所有图片的路径\n",
    "        self.image_paths = []\n",
    "        for filename in os.listdir(image_dir):\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                self.image_paths.append(os.path.join(image_dir, filename))\n",
    "\n",
    "        print(f\"图片已载入，共 {len(self.image_paths)} 张图片\")  # 提示图片已载入\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image_name = os.path.basename(image_path)\n",
    "\n",
    "        # 打开图像\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # 获取对应的年龄标签\n",
    "        if image_name in self.image_age_mapping:\n",
    "            age = self.image_age_mapping[image_name]\n",
    "        else:\n",
    "            raise ValueError(f\"未找到图片 {image_name} 的年龄标签信息\")\n",
    "\n",
    "        # 应用图像变换\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, age\n",
    "\n",
    "\n",
    "# 定义图像变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 将图像统一调整为224x224大小\n",
    "    transforms.RandomHorizontalFlip(),  # 随机水平翻转\n",
    "    transforms.ToTensor(),  # 转换为张量\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 标准化\n",
    "])\n",
    "\n",
    "# 创建训练集实例\n",
    "train_image_dir = r'C:\\Users\\1\\Desktop\\age\\trainset'\n",
    "train_csv_file = r'C:\\Users\\1\\Desktop\\age\\train.csv'\n",
    "train_dataset = AgeDataset(train_image_dir, train_csv_file, transform=transform)\n",
    "\n",
    "# 创建验证集实例\n",
    "val_image_dir = r'C:\\Users\\1\\Desktop\\age\\valset'\n",
    "val_csv_file = r'C:\\Users\\1\\Desktop\\age\\val.csv'\n",
    "val_dataset = AgeDataset(val_image_dir, val_csv_file, transform=transform)\n",
    "\n",
    "# 加载预训练的 ResNet10（通过 ResNet18 代替）\n",
    "model = models.resnet18(pretrained=True)  # 替换为 ResNet18\n",
    "num_ftrs = model.fc.in_features\n",
    "# 假设年龄预测是一个回归问题\n",
    "model.fc = nn.Linear(num_ftrs, 1)\n",
    "\n",
    "# 解冻更多的层（可以解冻整个网络，或者解冻部分层）\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "mae_criterion = nn.L1Loss()  # MAE 损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# 训练设备\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 8\n",
    "batch_size = 16  # 增加 batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n--- Epoch {epoch + 1}/{num_epochs} 开始 ---\")  # 输出周期开始提示\n",
    "    # 创建训练数据加载器，优化数据加载\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_mae = 0.0\n",
    "    all_errors = []\n",
    "    for images, ages in train_dataloader:\n",
    "        images, ages = images.to(device), ages.to(device).float().unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, ages)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        mae = mae_criterion(outputs, ages)\n",
    "        running_mae += mae.item()\n",
    "\n",
    "        # 计算每个样本的绝对误差\n",
    "        errors = torch.abs(outputs - ages).squeeze().cpu().tolist()\n",
    "        all_errors.extend(errors)\n",
    "\n",
    "    train_loss = running_loss / len(train_dataloader)\n",
    "    train_mae = running_mae / len(train_dataloader)\n",
    "\n",
    "    # 每 4 个 epoch 执行一次缩减\n",
    "    if (epoch + 1) % 4 == 0:\n",
    "        print(f\"--- 对数据集进行缩减 ---\")\n",
    "        num_to_remove = int(len(all_errors) * 0.05)\n",
    "        sorted_indices = sorted(range(len(all_errors)), key=lambda i: all_errors[i], reverse=True)\n",
    "        indices_to_remove = sorted_indices[:num_to_remove]\n",
    "        remaining_indices = [i for i in range(len(train_dataset)) if i not in indices_to_remove]\n",
    "        train_dataset = Subset(train_dataset, remaining_indices)  # 更新 train_dataset\n",
    "\n",
    "    # 验证模型\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_mae = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, ages in val_dataloader:\n",
    "            images, ages = images.to(device), ages.to(device).float().unsqueeze(1)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, ages)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            mae = mae_criterion(outputs, ages)\n",
    "            val_mae += mae.item()\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_mae /= len(val_dataloader)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train MAE: {train_mae:.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}')\n",
    "\n",
    "    # 调整学习率\n",
    "    scheduler.step()\n",
    "\n",
    "    # 保存模型\n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        torch.save(model.state_dict(), f'model_epoch_{epoch + 1}.pth')\n",
    "\n",
    "    print(f\"--- Epoch {epoch + 1}/{num_epochs} 结束 ---\")  # 输出周期结束提示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功载入 20000 张有效图片\n",
      "成功载入 3000 张有效图片\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_4788\\783569905.py:177: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_4788\\783569905.py:80: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/15\n",
      "Train >> Loss: 2.3407 | Acc: 0.1888 | MAE: 31.39\n",
      "Valid >> Loss: 2.1174 | Acc: 0.2413 | MAE: 26.80\n",
      "\n",
      "Epoch 2/15\n",
      "Train >> Loss: 2.1985 | Acc: 0.2235 | MAE: 27.60\n",
      "Valid >> Loss: 2.0871 | Acc: 0.2320 | MAE: 24.28\n",
      "\n",
      "Epoch 3/15\n",
      "Train >> Loss: 2.1307 | Acc: 0.2460 | MAE: 26.11\n",
      "Valid >> Loss: 2.1093 | Acc: 0.2300 | MAE: 22.89\n",
      "\n",
      "Epoch 4/15\n",
      "Train >> Loss: 2.0664 | Acc: 0.2651 | MAE: 24.86\n",
      "Valid >> Loss: 2.0807 | Acc: 0.2333 | MAE: 23.45\n",
      "Updating training dataset...\n",
      "\n",
      "Epoch 5/15\n",
      "Train >> Loss: 1.9927 | Acc: 0.2893 | MAE: 23.84\n",
      "Valid >> Loss: 2.1754 | Acc: 0.2200 | MAE: 23.63\n",
      "\n",
      "Epoch 6/15\n",
      "Train >> Loss: 1.9178 | Acc: 0.3214 | MAE: 22.83\n",
      "Valid >> Loss: 2.2162 | Acc: 0.2350 | MAE: 23.17\n",
      "\n",
      "Epoch 7/15\n",
      "Train >> Loss: 1.8096 | Acc: 0.3584 | MAE: 21.42\n",
      "Valid >> Loss: 2.2217 | Acc: 0.2267 | MAE: 24.82\n",
      "\n",
      "Epoch 8/15\n",
      "Train >> Loss: 1.5799 | Acc: 0.4582 | MAE: 18.73\n",
      "Valid >> Loss: 2.2108 | Acc: 0.2343 | MAE: 23.30\n",
      "Updating training dataset...\n",
      "\n",
      "Epoch 9/15\n",
      "Train >> Loss: 1.4654 | Acc: 0.5031 | MAE: 17.63\n",
      "Valid >> Loss: 2.2772 | Acc: 0.2307 | MAE: 23.55\n",
      "\n",
      "Epoch 10/15\n",
      "Train >> Loss: 1.3838 | Acc: 0.5363 | MAE: 16.81\n",
      "Valid >> Loss: 2.3232 | Acc: 0.2213 | MAE: 23.80\n",
      "\n",
      "Epoch 11/15\n",
      "Train >> Loss: 1.3227 | Acc: 0.5654 | MAE: 16.18\n",
      "Valid >> Loss: 2.3115 | Acc: 0.2317 | MAE: 23.43\n",
      "\n",
      "Epoch 12/15\n",
      "Train >> Loss: 1.3141 | Acc: 0.5619 | MAE: 16.07\n",
      "Valid >> Loss: 2.3240 | Acc: 0.2263 | MAE: 23.39\n",
      "Updating training dataset...\n",
      "\n",
      "Epoch 13/15\n",
      "Train >> Loss: 1.3052 | Acc: 0.5662 | MAE: 16.16\n",
      "Valid >> Loss: 2.3290 | Acc: 0.2250 | MAE: 23.45\n",
      "\n",
      "Epoch 14/15\n",
      "Train >> Loss: 1.2986 | Acc: 0.5698 | MAE: 15.90\n",
      "Valid >> Loss: 2.3264 | Acc: 0.2313 | MAE: 23.57\n",
      "\n",
      "Epoch 15/15\n",
      "Train >> Loss: 1.2991 | Acc: 0.5705 | MAE: 15.90\n",
      "Valid >> Loss: 2.3092 | Acc: 0.2327 | MAE: 23.42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, models\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import numpy as np\n",
    "\n",
    "# ------------------- 数据集类 -------------------\n",
    "class AgeDataset(Dataset):\n",
    "    def __init__(self, image_dir, csv_file, transform=None, age_bins=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.age_bins = np.array(age_bins)  # 使用numpy加速计算\n",
    "        self.image_info = []\n",
    "\n",
    "        # 读取CSV并过滤无效数据\n",
    "        with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                img_name = row['name']\n",
    "                img_path = os.path.join(image_dir, img_name)\n",
    "                if os.path.exists(img_path):\n",
    "                    age = int(row['age'])\n",
    "                    self.image_info.append((img_path, age))\n",
    "\n",
    "        print(f\"成功载入 {len(self.image_info)} 张有效图片\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, age = self.image_info[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # 自动计算年龄类别（利用numpy向量化加速）\n",
    "        age_class = np.digitize(age, self.age_bins, right=False) - 1\n",
    "        age_class = max(0, min(age_class, len(self.age_bins)-2))  # 确保不越界\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(age, dtype=torch.float32), torch.tensor(age_class)\n",
    "\n",
    "# ------------------- 模型定义 -------------------\n",
    "class AgeResNet50(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # 加载预训练ResNet50\n",
    "        self.base = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # 冻结底层参数（可选）\n",
    "        # for param in self.base.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        \n",
    "        # 替换全连接层\n",
    "        in_features = self.base.fc.in_features\n",
    "        self.base.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base(x)\n",
    "\n",
    "# ------------------- 训练工具函数 -------------------\n",
    "def train_epoch(model, loader, criterion, optimizer, device, scaler, age_bins):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_mae = 0, 0, 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for images, ages, classes in loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        classes = classes.to(device, non_blocking=True)\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        # 混合精度训练\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, classes)\n",
    "\n",
    "        # 反向传播\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # 计算指标\n",
    "        total_loss += loss.item() * batch_size\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        total_correct += (preds == classes).sum().item()\n",
    "\n",
    "        # 向量化MAE计算\n",
    "        pred_ages = torch.tensor([age_bins[pred] for pred in preds.cpu().numpy()]).to(device)\n",
    "        total_mae += torch.abs(pred_ages - ages.to(device)).sum().item()\n",
    "        total_samples += batch_size\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / total_samples,\n",
    "        'acc': total_correct / total_samples,\n",
    "        'mae': total_mae / total_samples\n",
    "    }\n",
    "\n",
    "def validate(model, loader, criterion, device, age_bins):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_mae = 0, 0, 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, ages, classes in loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            classes = classes.to(device, non_blocking=True)\n",
    "            batch_size = images.size(0)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, classes)\n",
    "\n",
    "            total_loss += loss.item() * batch_size\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            total_correct += (preds == classes).sum().item()\n",
    "\n",
    "            # 向量化MAE计算\n",
    "            pred_ages = torch.tensor([age_bins[pred] for pred in preds.cpu().numpy()]).to(device)\n",
    "            total_mae += torch.abs(pred_ages - ages.to(device)).sum().item()\n",
    "            total_samples += batch_size\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / total_samples,\n",
    "        'acc': total_correct / total_samples,\n",
    "        'mae': total_mae / total_samples\n",
    "    }\n",
    "\n",
    "# ------------------- 主训练流程 -------------------\n",
    "def main():\n",
    "    # 配置参数\n",
    "    # 定义年龄区间，每 12 岁一个区间，覆盖 [0, 192) 区间\n",
    "    age_bins = list(range(0, 193, 12))  # [0, 12), [12, 24), ..., [180, 192)\n",
    "    batch_size = 128  # ResNet50需要减小batch size\n",
    "    num_epochs = 15\n",
    "    lr = 1e-4\n",
    "\n",
    "    # 数据增强\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(0.2, 0.2, 0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # 数据集\n",
    "    train_dataset = AgeDataset(r'C:\\Users\\1\\Desktop\\age\\trainset', 'train.csv', train_transform, age_bins)\n",
    "    val_dataset = AgeDataset(r'C:\\Users\\1\\Desktop\\age\\valset', 'val.csv', val_transform, age_bins)\n",
    "\n",
    "    # 数据加载器\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                             shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                           num_workers=0, pin_memory=True)\n",
    "\n",
    "    # 模型初始化\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = AgeResNet50(num_classes=len(age_bins)-1).to(device)  # 输出类别数要少 1，因为区间是左闭右开\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # 训练循环\n",
    "    best_mae = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练阶段\n",
    "        train_metrics = train_epoch(model, train_loader, criterion, optimizer, device, scaler, age_bins)\n",
    "        \n",
    "        # 验证阶段\n",
    "        val_metrics = validate(model, val_loader, criterion, device, age_bins)\n",
    "        scheduler.step(val_metrics['loss'])\n",
    "\n",
    "        # 打印结果\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train >> Loss: {train_metrics['loss']:.4f} | Acc: {train_metrics['acc']:.4f} | MAE: {train_metrics['mae']:.2f}\")\n",
    "        print(f\"Valid >> Loss: {val_metrics['loss']:.4f} | Acc: {val_metrics['acc']:.4f} | MAE: {val_metrics['mae']:.2f}\")\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if val_metrics['mae'] < best_mae:\n",
    "            best_mae = val_metrics['mae']\n",
    "            torch.save(model.state_dict(), f'best_model_mae{best_mae:.2f}.pth')\n",
    "\n",
    "        # 每4个epoch缩减数据集\n",
    "        if (epoch+1) % 4 == 0:\n",
    "            print(\"Updating training dataset...\")\n",
    "            # 这里添加你的数据集更新逻辑\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功载入 20000 张有效图片\n",
      "成功载入 3000 张有效图片\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_20368\\2464312120.py:213: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_20368\\2464312120.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "Train >> Loss: 3500.2870 | Acc: 0.1570 | MAE: 46.41\n",
      "Valid >> Loss: 2311.0773 | Acc: 0.2297 | MAE: 36.27\n",
      "\n",
      "Epoch 2/20\n",
      "Train >> Loss: 1858.4375 | Acc: 0.1872 | MAE: 30.97\n",
      "Valid >> Loss: 1220.3385 | Acc: 0.2230 | MAE: 23.93\n",
      "\n",
      "Epoch 3/20\n",
      "Train >> Loss: 1190.0335 | Acc: 0.1888 | MAE: 24.49\n",
      "Valid >> Loss: 1028.6966 | Acc: 0.1870 | MAE: 24.55\n",
      "\n",
      "Epoch 4/20\n",
      "Train >> Loss: 1019.0925 | Acc: 0.1965 | MAE: 23.12\n",
      "Valid >> Loss: 915.0897 | Acc: 0.2337 | MAE: 22.24\n",
      "\n",
      "Epoch 5/20\n",
      "Train >> Loss: 959.5208 | Acc: 0.1950 | MAE: 22.64\n",
      "Valid >> Loss: 901.9238 | Acc: 0.2410 | MAE: 21.62\n",
      "\n",
      "Epoch 6/20\n",
      "Train >> Loss: 911.9181 | Acc: 0.2057 | MAE: 22.11\n",
      "Valid >> Loss: 922.8781 | Acc: 0.2177 | MAE: 22.31\n",
      "\n",
      "Epoch 7/20\n",
      "Train >> Loss: 854.9123 | Acc: 0.2041 | MAE: 21.35\n",
      "Valid >> Loss: 958.0239 | Acc: 0.2260 | MAE: 23.35\n",
      "\n",
      "Epoch 8/20\n",
      "Train >> Loss: 793.7935 | Acc: 0.2135 | MAE: 20.64\n",
      "Valid >> Loss: 963.1448 | Acc: 0.2403 | MAE: 22.30\n",
      "\n",
      "Epoch 9/20\n",
      "Train >> Loss: 680.1397 | Acc: 0.2246 | MAE: 19.01\n",
      "Valid >> Loss: 904.8776 | Acc: 0.2387 | MAE: 21.51\n",
      "\n",
      "Epoch 10/20\n",
      "Train >> Loss: 629.8506 | Acc: 0.2291 | MAE: 18.38\n",
      "Valid >> Loss: 909.4848 | Acc: 0.2407 | MAE: 21.56\n",
      "\n",
      "Epoch 11/20\n",
      "Train >> Loss: 601.5889 | Acc: 0.2362 | MAE: 18.03\n",
      "Valid >> Loss: 917.6612 | Acc: 0.2347 | MAE: 21.79\n",
      "\n",
      "Epoch 12/20\n",
      "Train >> Loss: 590.9757 | Acc: 0.2318 | MAE: 17.74\n",
      "Valid >> Loss: 906.4161 | Acc: 0.2380 | MAE: 21.52\n",
      "\n",
      "Epoch 13/20\n",
      "Train >> Loss: 579.4247 | Acc: 0.2389 | MAE: 17.64\n",
      "Valid >> Loss: 914.8542 | Acc: 0.2390 | MAE: 21.57\n",
      "\n",
      "Epoch 14/20\n",
      "Train >> Loss: 579.9248 | Acc: 0.2371 | MAE: 17.61\n",
      "Valid >> Loss: 908.8590 | Acc: 0.2387 | MAE: 21.48\n",
      "\n",
      "Epoch 15/20\n",
      "Train >> Loss: 566.1741 | Acc: 0.2382 | MAE: 17.52\n",
      "Valid >> Loss: 917.0286 | Acc: 0.2353 | MAE: 21.63\n",
      "\n",
      "Epoch 16/20\n",
      "Train >> Loss: 574.2077 | Acc: 0.2412 | MAE: 17.60\n",
      "Valid >> Loss: 910.0068 | Acc: 0.2303 | MAE: 21.61\n",
      "\n",
      "Epoch 17/20\n",
      "Train >> Loss: 568.1914 | Acc: 0.2366 | MAE: 17.46\n",
      "Valid >> Loss: 909.5533 | Acc: 0.2343 | MAE: 21.49\n",
      "\n",
      "Epoch 18/20\n",
      "Train >> Loss: 570.3481 | Acc: 0.2391 | MAE: 17.48\n",
      "Valid >> Loss: 905.6193 | Acc: 0.2380 | MAE: 21.34\n",
      "\n",
      "Epoch 19/20\n",
      "Train >> Loss: 575.5069 | Acc: 0.2362 | MAE: 17.67\n",
      "Valid >> Loss: 914.3700 | Acc: 0.2377 | MAE: 21.53\n",
      "\n",
      "Epoch 20/20\n",
      "Train >> Loss: 569.7081 | Acc: 0.2346 | MAE: 17.60\n",
      "Valid >> Loss: 942.3939 | Acc: 0.2337 | MAE: 22.11\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, models\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import numpy as np\n",
    "\n",
    "# ------------------- 数据集类 -------------------\n",
    "class AgeDataset(Dataset):\n",
    "    def __init__(self, image_dir, csv_file, transform=None, age_bins=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.age_bins = np.array(age_bins)\n",
    "        \n",
    "        # 使用pandas读取CSV\n",
    "        self.image_info = []\n",
    "        df = pd.read_csv(csv_file)\n",
    "        for _, row in df.iterrows():\n",
    "            img_name = row['name']\n",
    "            img_path = os.path.join(image_dir, img_name)\n",
    "            if os.path.exists(img_path):\n",
    "                age = int(row['age'])\n",
    "                self.image_info.append((img_path, age))\n",
    "        \n",
    "        print(f\"成功载入 {len(self.image_info)} 张有效图片\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, age = self.image_info[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # 自动计算年龄类别\n",
    "        age_class = np.digitize(age, self.age_bins, right=False) - 1\n",
    "        age_class = max(0, min(age_class, len(self.age_bins)-2)) \n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(age, dtype=torch.float32), torch.tensor(age_class)\n",
    "\n",
    "# ------------------- 模型定义 -------------------\n",
    "class AgeResNet50(nn.Module):\n",
    "    def __init__(self, num_classes, is_regression=False):\n",
    "        super().__init__()\n",
    "        # 加载预训练ResNet50\n",
    "        self.base = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # 冻结底层参数（可选）\n",
    "        # for param in self.base.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        \n",
    "        # 替换全连接层（用于分类）\n",
    "        in_features = self.base.fc.in_features\n",
    "        self.base.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features, num_classes)  # 分类到不同的年龄区间\n",
    "        )\n",
    "        \n",
    "        # 回归任务的输出层\n",
    "        self.regression_fc = nn.Linear(in_features, 1)  # 输出具体年龄\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 通过ResNet50提取特征\n",
    "        x = self.base.conv1(x)\n",
    "        x = self.base.bn1(x)\n",
    "        x = self.base.relu(x)\n",
    "        x = self.base.maxpool(x)\n",
    "        \n",
    "        x = self.base.layer1(x)\n",
    "        x = self.base.layer2(x)\n",
    "        x = self.base.layer3(x)\n",
    "        x = self.base.layer4(x)\n",
    "        \n",
    "        # 得到分类的结果 (x的shape是(batch_size, 2048))\n",
    "        x_classification = self.base.avgpool(x)\n",
    "        x_classification = torch.flatten(x_classification, 1)  # 将特征展平\n",
    "        \n",
    "        # 分类任务输出\n",
    "        classification_output = self.base.fc(x_classification)\n",
    "        \n",
    "        # 回归部分\n",
    "        regression_output = self.regression_fc(x_classification)\n",
    "        \n",
    "        return classification_output, regression_output\n",
    "\n",
    "# ------------------- 训练工具函数 -------------------\n",
    "def train_epoch(model, loader, criterion_class, criterion_regression, optimizer, device, scaler, age_bins):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_mae = 0, 0, 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for images, ages, classes in loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        classes = classes.to(device, non_blocking=True)\n",
    "        ages = ages.to(device, non_blocking=True)\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        # 混合精度训练\n",
    "        with autocast():\n",
    "            # 分类和回归的输出\n",
    "            class_outputs, reg_outputs = model(images)\n",
    "            \n",
    "            # 分类任务损失\n",
    "            class_loss = criterion_class(class_outputs, classes)\n",
    "            \n",
    "            # 回归任务损失\n",
    "            reg_loss = criterion_regression(reg_outputs.squeeze(), ages)\n",
    "            \n",
    "            # 总损失\n",
    "            loss = class_loss + reg_loss\n",
    "\n",
    "        # 反向传播\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        total_loss += loss.item() * batch_size\n",
    "        preds = class_outputs.argmax(dim=1)\n",
    "        total_correct += (preds == classes).sum().item()\n",
    "\n",
    "        total_mae += torch.abs(reg_outputs.squeeze() - ages).sum().item()\n",
    "        total_samples += batch_size\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    mae = total_mae / total_samples\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / total_samples,\n",
    "        'acc': accuracy,\n",
    "        'mae': mae\n",
    "    }\n",
    "\n",
    "def validate(model, loader, criterion_class, criterion_regression, device, age_bins):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_mae = 0, 0, 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, ages, classes in loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            classes = classes.to(device, non_blocking=True)\n",
    "            ages = ages.to(device, non_blocking=True)\n",
    "            batch_size = images.size(0)\n",
    "\n",
    "            # 分类和回归的输出\n",
    "            class_outputs, reg_outputs = model(images)\n",
    "            \n",
    "            class_loss = criterion_class(class_outputs, classes)\n",
    "            reg_loss = criterion_regression(reg_outputs.squeeze(), ages)\n",
    "            loss = class_loss + reg_loss\n",
    "\n",
    "            total_loss += loss.item() * batch_size\n",
    "            preds = class_outputs.argmax(dim=1)\n",
    "            total_correct += (preds == classes).sum().item()\n",
    "\n",
    "            total_mae += torch.abs(reg_outputs.squeeze() - ages).sum().item()\n",
    "            total_samples += batch_size\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    mae = total_mae / total_samples\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / total_samples,\n",
    "        'acc': accuracy,\n",
    "        'mae': mae\n",
    "    }\n",
    "\n",
    "# ------------------- 主训练流程 -------------------\n",
    "def main():\n",
    "    age_bins = list(range(0, 193, 12))  # 0-192岁，每12岁一个区间\n",
    "    batch_size = 128\n",
    "    num_epochs = 20\n",
    "    lr = 1e-4\n",
    "\n",
    "    # 数据增强\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(0.2, 0.2, 0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    train_dataset = AgeDataset(r'C:\\Users\\1\\Desktop\\age\\trainset', 'train.csv', train_transform, age_bins)\n",
    "    val_dataset = AgeDataset(r'C:\\Users\\1\\Desktop\\age\\valset', 'val.csv', val_transform, age_bins)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=0, pin_memory=True)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = AgeResNet50(num_classes=len(age_bins)).to(device)\n",
    "    \n",
    "    criterion_class = nn.CrossEntropyLoss()\n",
    "    criterion_regression = nn.MSELoss()\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    best_mae = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练阶段\n",
    "        train_metrics = train_epoch(model, train_loader, criterion_class, criterion_regression, optimizer, device, scaler, age_bins)\n",
    "        \n",
    "        # 验证阶段\n",
    "        val_metrics = validate(model, val_loader, criterion_class, criterion_regression, device, age_bins)\n",
    "        scheduler.step(val_metrics['loss'])\n",
    "\n",
    "        # 打印结果\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train >> Loss: {train_metrics['loss']:.4f} | Acc: {train_metrics['acc']:.4f} | MAE: {train_metrics['mae']:.2f}\")\n",
    "        print(f\"Valid >> Loss: {val_metrics['loss']:.4f} | Acc: {val_metrics['acc']:.4f} | MAE: {val_metrics['mae']:.2f}\")\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if val_metrics['mae'] < best_mae:\n",
    "            best_mae = val_metrics['mae']\n",
    "            torch.save(model.state_dict(), f'best_model_mae{best_mae:.2f}.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功载入 20000 张有效图片\n",
      "成功载入 3000 张有效图片\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_11464\\3191942214.py:237: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_11464\\3191942214.py:107: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "Train >> Loss: 3381.5045 | Acc: 0.1580 | MAE: 45.50\n",
      "Valid >> Loss: 2090.8492 | Acc: 0.1990 | MAE: 33.75\n",
      "\n",
      "Epoch 2/20\n",
      "Train >> Loss: 1846.4062 | Acc: 0.1757 | MAE: 30.91\n",
      "Valid >> Loss: 1111.4431 | Acc: 0.1770 | MAE: 24.87\n",
      "\n",
      "Epoch 3/20\n",
      "Train >> Loss: 1253.9637 | Acc: 0.1787 | MAE: 25.39\n",
      "Valid >> Loss: 1012.7089 | Acc: 0.2103 | MAE: 23.73\n",
      "\n",
      "Epoch 4/20\n",
      "Train >> Loss: 1103.1221 | Acc: 0.1890 | MAE: 24.30\n",
      "Valid >> Loss: 1010.7983 | Acc: 0.2173 | MAE: 23.67\n",
      "删除 800 个 MAE 差值最大的样本\n",
      "\n",
      "Epoch 5/20\n",
      "Train >> Loss: 1041.8170 | Acc: 0.1963 | MAE: 23.77\n",
      "Valid >> Loss: 993.8233 | Acc: 0.2297 | MAE: 23.02\n",
      "\n",
      "Epoch 6/20\n",
      "Train >> Loss: 1160.8722 | Acc: 0.1760 | MAE: 25.41\n",
      "Valid >> Loss: 1007.6815 | Acc: 0.2223 | MAE: 24.29\n",
      "\n",
      "Epoch 7/20\n",
      "Train >> Loss: 1076.5542 | Acc: 0.1890 | MAE: 24.41\n",
      "Valid >> Loss: 958.3063 | Acc: 0.2093 | MAE: 22.55\n",
      "\n",
      "Epoch 8/20\n",
      "Train >> Loss: 1040.3639 | Acc: 0.1974 | MAE: 23.90\n",
      "Valid >> Loss: 965.1147 | Acc: 0.2213 | MAE: 22.75\n",
      "\n",
      "Epoch 9/20\n",
      "Train >> Loss: 1019.7236 | Acc: 0.1965 | MAE: 23.58\n",
      "Valid >> Loss: 979.7233 | Acc: 0.2047 | MAE: 24.17\n",
      "删除 768 个 MAE 差值最大的样本\n",
      "\n",
      "Epoch 10/20\n",
      "Train >> Loss: 982.2293 | Acc: 0.2012 | MAE: 23.15\n",
      "Valid >> Loss: 923.2033 | Acc: 0.2303 | MAE: 22.29\n",
      "\n",
      "Epoch 11/20\n",
      "Train >> Loss: 940.3846 | Acc: 0.2090 | MAE: 22.55\n",
      "Valid >> Loss: 931.3725 | Acc: 0.2280 | MAE: 22.33\n",
      "\n",
      "Epoch 12/20\n",
      "Train >> Loss: 898.2074 | Acc: 0.2110 | MAE: 22.17\n",
      "Valid >> Loss: 937.8002 | Acc: 0.2300 | MAE: 21.80\n",
      "\n",
      "Epoch 13/20\n",
      "Train >> Loss: 887.0098 | Acc: 0.2127 | MAE: 21.92\n",
      "Valid >> Loss: 935.7752 | Acc: 0.2297 | MAE: 21.67\n",
      "\n",
      "Epoch 14/20\n",
      "Train >> Loss: 763.0841 | Acc: 0.2242 | MAE: 20.26\n",
      "Valid >> Loss: 919.6253 | Acc: 0.2470 | MAE: 21.96\n",
      "删除 737 个 MAE 差值最大的样本\n",
      "\n",
      "Epoch 15/20\n",
      "Train >> Loss: 703.4870 | Acc: 0.2323 | MAE: 19.44\n",
      "Valid >> Loss: 917.4157 | Acc: 0.2430 | MAE: 21.73\n",
      "\n",
      "Epoch 16/20\n",
      "Train >> Loss: 664.6432 | Acc: 0.2339 | MAE: 18.99\n",
      "Valid >> Loss: 928.7992 | Acc: 0.2403 | MAE: 21.78\n",
      "\n",
      "Epoch 17/20\n",
      "Train >> Loss: 644.9986 | Acc: 0.2385 | MAE: 18.57\n",
      "Valid >> Loss: 916.5598 | Acc: 0.2430 | MAE: 21.59\n",
      "\n",
      "Epoch 18/20\n",
      "Train >> Loss: 636.6935 | Acc: 0.2378 | MAE: 18.52\n",
      "Valid >> Loss: 910.4000 | Acc: 0.2347 | MAE: 21.47\n",
      "\n",
      "Epoch 19/20\n",
      "Train >> Loss: 607.7779 | Acc: 0.2439 | MAE: 18.08\n",
      "Valid >> Loss: 937.1656 | Acc: 0.2380 | MAE: 22.02\n",
      "删除 707 个 MAE 差值最大的样本\n",
      "\n",
      "Epoch 20/20\n",
      "Train >> Loss: 586.6308 | Acc: 0.2411 | MAE: 17.88\n",
      "Valid >> Loss: 943.7836 | Acc: 0.2353 | MAE: 22.05\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, models\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import numpy as np\n",
    "\n",
    "# ------------------- 数据集类 -------------------\n",
    "class AgeDataset(Dataset):\n",
    "    def __init__(self, image_dir, csv_file, transform=None, age_bins=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.age_bins = np.array(age_bins)\n",
    "        \n",
    "        # 使用pandas读取CSV\n",
    "        self.image_info = []\n",
    "        df = pd.read_csv(csv_file)\n",
    "        for _, row in df.iterrows():\n",
    "            img_name = row['name']\n",
    "            img_path = os.path.join(image_dir, img_name)\n",
    "            if os.path.exists(img_path):\n",
    "                age = int(row['age'])\n",
    "                self.image_info.append((img_path, age))\n",
    "        \n",
    "        print(f\"成功载入 {len(self.image_info)} 张有效图片\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, age = self.image_info[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # 自动计算年龄类别\n",
    "        age_class = np.digitize(age, self.age_bins, right=False) - 1\n",
    "        age_class = max(0, min(age_class, len(self.age_bins)-2))  # 防止越界\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(age, dtype=torch.float32), torch.tensor(age_class)\n",
    "\n",
    "    def remove_sample(self, idx):\n",
    "        \"\"\" Remove a sample from dataset by index \"\"\"\n",
    "        self.image_info.pop(idx)\n",
    "\n",
    "# ------------------- 模型定义 -------------------\n",
    "class AgeResNet50(nn.Module):\n",
    "    def __init__(self, num_classes, is_regression=False):\n",
    "        super().__init__()\n",
    "        # 加载预训练ResNet152\n",
    "        self.base = models.resnet152(pretrained=True)\n",
    "        \n",
    "        # 替换全连接层（用于分类）\n",
    "        in_features = self.base.fc.in_features\n",
    "        self.base.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features, num_classes)  # 分类到不同的年龄区间\n",
    "        )\n",
    "        \n",
    "        # 回归任务的输出层\n",
    "        self.regression_fc = nn.Linear(in_features, 1)  # 输出具体年龄\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 通过ResNet152提取特征\n",
    "        x = self.base.conv1(x)\n",
    "        x = self.base.bn1(x)\n",
    "        x = self.base.relu(x)\n",
    "        x = self.base.maxpool(x)\n",
    "        \n",
    "        x = self.base.layer1(x)\n",
    "        x = self.base.layer2(x)\n",
    "        x = self.base.layer3(x)\n",
    "        x = self.base.layer4(x)\n",
    "        \n",
    "        # 得到分类的结果 (x的shape是(batch_size, 2048))\n",
    "        x_classification = self.base.avgpool(x)\n",
    "        x_classification = torch.flatten(x_classification, 1)  # 将特征展平\n",
    "        \n",
    "        # 分类任务输出\n",
    "        classification_output = self.base.fc(x_classification)\n",
    "        \n",
    "        # 回归部分\n",
    "        regression_output = self.regression_fc(x_classification)\n",
    "        \n",
    "        return classification_output, regression_output\n",
    "\n",
    "# ------------------- 训练工具函数 -------------------\n",
    "def train_epoch(model, loader, criterion_class, criterion_regression, optimizer, device, scaler, age_bins, epoch, remove_invalid_samples=False):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_mae = 0, 0, 0\n",
    "    total_samples = 0\n",
    "    mae_differences = []  # 用于存储所有样本的 MAE 差值\n",
    "    sample_indices = []  # 用于存储样本索引，方便删除样本时对应索引\n",
    "\n",
    "    for idx, (images, ages, classes) in enumerate(loader):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        classes = classes.to(device, non_blocking=True)\n",
    "        ages = ages.to(device, non_blocking=True)\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        # 混合精度训练\n",
    "        with autocast():\n",
    "            # 分类和回归的输出\n",
    "            class_outputs, reg_outputs = model(images)\n",
    "            \n",
    "            # 分类任务损失\n",
    "            class_loss = criterion_class(class_outputs, classes)\n",
    "            \n",
    "            # 回归任务损失\n",
    "            reg_loss = criterion_regression(reg_outputs.squeeze(), ages)\n",
    "            \n",
    "            # 总损失\n",
    "            loss = class_loss + reg_loss\n",
    "\n",
    "        # 反向传播\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        total_loss += loss.item() * batch_size\n",
    "        preds = class_outputs.argmax(dim=1)\n",
    "        total_correct += (preds == classes).sum().item()\n",
    "\n",
    "        total_mae += torch.abs(reg_outputs.squeeze() - ages).sum().item()\n",
    "        total_samples += batch_size\n",
    "\n",
    "        # 计算MAE差值并存储（确保不计算梯度）\n",
    "        mae_differences.extend(torch.abs(reg_outputs.squeeze() - ages).detach().cpu().numpy())\n",
    "        sample_indices.extend([idx] * batch_size)  # 记录当前批次样本的索引\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    mae = total_mae / total_samples\n",
    "\n",
    "    # 每5个周期进行检查并删除MAE差值最大的5%的样本\n",
    "    if epoch % 5 == 4 and remove_invalid_samples:\n",
    "        # 计算前3% MAE差值最大的样本的索引\n",
    "        threshold = int(0.04 * len(mae_differences))  # 删除最大3%的样本\n",
    "        sorted_indices = np.argsort(mae_differences)[::-1]  # 按照MAE差值降序排序\n",
    "        remove_indices = sorted_indices[:threshold]  # 获取最大的3%样本的索引\n",
    "\n",
    "        print(f\"删除 {len(remove_indices)} 个 MAE 差值最大的样本\")\n",
    "        \n",
    "        # 删除样本\n",
    "        for idx in sorted(remove_indices, reverse=True):  # 反向排序避免索引问题\n",
    "            loader.dataset.remove_sample(idx)\n",
    "\n",
    "        # 重新创建DataLoader，以更新数据集\n",
    "        global train_loader\n",
    "        train_loader = DataLoader(loader.dataset, batch_size=32, shuffle=True, num_workers=0, pin_memory=False)\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / total_samples,\n",
    "        'acc': accuracy,\n",
    "        'mae': mae\n",
    "    }\n",
    "\n",
    "def validate(model, loader, criterion_class, criterion_regression, device, age_bins):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_mae = 0, 0, 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, ages, classes in loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            classes = classes.to(device, non_blocking=True)\n",
    "            ages = ages.to(device, non_blocking=True)\n",
    "            batch_size = images.size(0)\n",
    "\n",
    "            # 分类和回归的输出\n",
    "            class_outputs, reg_outputs = model(images)\n",
    "            \n",
    "            class_loss = criterion_class(class_outputs, classes)\n",
    "            reg_loss = criterion_regression(reg_outputs.squeeze(), ages)\n",
    "            loss = class_loss + reg_loss\n",
    "\n",
    "            total_loss += loss.item() * batch_size\n",
    "            preds = class_outputs.argmax(dim=1)\n",
    "            total_correct += (preds == classes).sum().item()\n",
    "\n",
    "            total_mae += torch.abs(reg_outputs.squeeze() - ages).sum().item()\n",
    "            total_samples += batch_size\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    mae = total_mae / total_samples\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / total_samples,\n",
    "        'acc': accuracy,\n",
    "        'mae': mae\n",
    "    }\n",
    "\n",
    "# ------------------- 主训练流程 -------------------\n",
    "def main():\n",
    "    age_bins = list(range(0, 193, 12))  # 0-192岁，每12岁一个区间\n",
    "    batch_size = 128\n",
    "    num_epochs = 20\n",
    "    lr = 1e-4\n",
    "\n",
    "    # 数据增强\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(0.2, 0.2, 0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    train_dataset = AgeDataset(r'C:\\Users\\1\\Desktop\\age\\trainset', 'train.csv', train_transform, age_bins)\n",
    "    val_dataset = AgeDataset(r'C:\\Users\\1\\Desktop\\age\\valset', 'val.csv', val_transform, age_bins)\n",
    "\n",
    "    global train_loader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=0, pin_memory=False)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = AgeResNet50(num_classes=len(age_bins)).to(device)\n",
    "    \n",
    "    criterion_class = nn.CrossEntropyLoss()\n",
    "    criterion_regression = nn.MSELoss()\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    best_mae = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练阶段\n",
    "        train_metrics = train_epoch(model, train_loader, criterion_class, criterion_regression, optimizer, device, scaler, age_bins, epoch, remove_invalid_samples=True)\n",
    "        \n",
    "        # 验证阶段\n",
    "        val_metrics = validate(model, val_loader, criterion_class, criterion_regression, device, age_bins)\n",
    "        scheduler.step(val_metrics['loss'])\n",
    "\n",
    "        # 打印结果，使用 f-string 格式化输出\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train >> Loss: {train_metrics['loss']:.4f} | Acc: {train_metrics['acc']:.4f} | MAE: {train_metrics['mae']:.2f}\")\n",
    "        print(f\"Valid >> Loss: {val_metrics['loss']:.4f} | Acc: {val_metrics['acc']:.4f} | MAE: {val_metrics['mae']:.2f}\")\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if val_metrics['mae'] < best_mae:\n",
    "            best_mae = val_metrics['mae']\n",
    "            torch.save(model.state_dict(), f'best_model_mae{best_mae:.2f}.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_14084\\2984098277.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(pth_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测结果已保存到 pred_result.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "\n",
    "# ------------------- 模型定义 -------------------\n",
    "class AgeResNet152(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # 使用ResNet152（确保训练和预测时使用一致的网络结构）\n",
    "        self.base = models.resnet152(pretrained=False)\n",
    "        in_features = self.base.fc.in_features\n",
    "        self.base.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features, num_classes)  # 分类分支\n",
    "        )\n",
    "        # 回归分支输出具体年龄\n",
    "        self.regression_fc = nn.Linear(in_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base.conv1(x)\n",
    "        x = self.base.bn1(x)\n",
    "        x = self.base.relu(x)\n",
    "        x = self.base.maxpool(x)\n",
    "        \n",
    "        x = self.base.layer1(x)\n",
    "        x = self.base.layer2(x)\n",
    "        x = self.base.layer3(x)\n",
    "        x = self.base.layer4(x)\n",
    "        \n",
    "        x_classification = self.base.avgpool(x)\n",
    "        x_classification = torch.flatten(x_classification, 1)\n",
    "        \n",
    "        classification_output = self.base.fc(x_classification)\n",
    "        regression_output = self.regression_fc(x_classification)\n",
    "        \n",
    "        return classification_output, regression_output\n",
    "\n",
    "# ------------------- 预测函数 -------------------\n",
    "def predict_ages(model, device, transform, test_dir, output_txt):\n",
    "    # 获取 test_dir 下所有图片文件（支持 .png, .jpg, .jpeg 格式）\n",
    "    image_files = [f for f in os.listdir(test_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    image_files.sort()  # 排序，确保输出顺序一致\n",
    "\n",
    "    results = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for img_file in image_files:\n",
    "            img_path = os.path.join(test_dir, img_file)\n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "            except Exception as e:\n",
    "                print(f\"读取图片 {img_file} 失败：{e}\")\n",
    "                continue\n",
    "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            _, reg_output = model(image_tensor)\n",
    "            predicted_age = reg_output.item()\n",
    "            # 四舍五入取整\n",
    "            predicted_age = int(round(predicted_age))\n",
    "            # 使用制表符分隔，符合格式要求\n",
    "            results.append(f\"{img_file}\\t{predicted_age}\")\n",
    "\n",
    "    # 将预测结果写入文本文件\n",
    "    with open(output_txt, 'w') as f:\n",
    "        for line in results:\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "    print(f\"预测结果已保存到 {output_txt}\")\n",
    "\n",
    "# ------------------- 主函数 -------------------\n",
    "def main():\n",
    "    test_dir = r\"C:\\Users\\1\\Desktop\\age\\valset\"      # 测试图片目录\n",
    "    output_txt = \"pred_result.txt\"                     # 输出结果文件名\n",
    "\n",
    "    # 定义测试时的数据预处理（与验证时一致）\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 使用训练时的年龄分箱，假设训练时使用的是 list(range(0, 193, 12))\n",
    "    age_bins = list(range(0, 193, 12))\n",
    "    num_classes = len(age_bins)\n",
    "\n",
    "    # 构造模型并加载权重（确保模型架构和训练时一致，此处使用ResNet152）\n",
    "    model = AgeResNet152(num_classes=num_classes).to(device)\n",
    "    # 修改下方 pth_path 为你保存的模型文件路径\n",
    "    pth_path = r\"C:\\Users\\1\\Desktop\\age\\best_model_mae21.47.pth\"\n",
    "    state_dict = torch.load(pth_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # 执行预测\n",
    "    predict_ages(model, device, transform, test_dir, output_txt)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
